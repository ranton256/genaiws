{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51629972-941e-484e-870f-455cc33d7a7e",
   "metadata": {},
   "source": [
    "# DDW GenAI Workshop Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79324077-4ab2-48d5-89c2-a8598a013762",
   "metadata": {},
   "source": [
    "## Jupyter Basics\n",
    "\n",
    "This is a Jupyter notebook.\n",
    "It is made up of cells that contain either Markdown text or Python code.\n",
    "\n",
    "Code cells can be executed.\n",
    "To select a cell click it with the mouse.\n",
    "\n",
    "To execute the currently selected cell click the Play Icon in the toolbar at the top of the page or hold the Shift key and press Enter.\n",
    "When you execute a text cell it renders the markdown.\n",
    "When you execute a code cell it executes the Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b4e14-b785-4507-96e4-fe2610c729de",
   "metadata": {},
   "source": [
    "## Check for GPU or Metal acceleration\n",
    "\n",
    "If you only have CPU available, you may want to stick with gemma:2b or other smaller models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26aa3cd0-8b87-4f79-92df-eddb57187b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch available device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    MY_DEVICE = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    MY_DEVICE = \"mps\"\n",
    "else:\n",
    "    MY_DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"pytorch available device is {MY_DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607PyB8beYtz",
   "metadata": {
    "id": "607PyB8beYtz"
   },
   "source": [
    "## Start Ollama Server\n",
    "\n",
    "Ollama lets you run LLMs locally on your own computer.\n",
    "\n",
    "- You should already have Ollama installed.\n",
    "  - If not you can download the installer from <https://ollama.com>.\n",
    "- Make sure the Ollama server is running.\n",
    "  - On Mac it will show as a Llama head icon in the menu bar, right hand side.\n",
    "  - On Windows it will show in the Task Manager area as an icon.\n",
    "- Start Ollama server if needed.\n",
    "  - On Mac, open Spotlight (click the magnifying glass in top right corner or press Command-Space bar) and search for \"Ollama.app\" and select it.\n",
    "  - On Windows click in the search box at the bottom of the screen and type Ollama, then click the Ollama app result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T1IdgMYHevH2",
   "metadata": {
    "id": "T1IdgMYHevH2"
   },
   "source": [
    "## Models\n",
    "\n",
    "The models we are going to be using with Ollama are:\n",
    "\n",
    "### Large Language Models\n",
    "- [Mistral](https://ollama.com/library/mistral) - Takes 4.1 gb\n",
    "- [Gemma:2b](https://ollama.com/library/gemma:2b) - About 2 gb\n",
    "- OPTIONAL (largest) [Llama3](https://ollama.com/library/llama3) - About 4.7 gb\n",
    "\n",
    "### Embedding Models\n",
    "- [Snowflake Arctic Embed](https://ollama.com/library/snowflake-arctic-embed) - About 700 mb\n",
    "- [Nomic Embed Text](https://ollama.com/library/nomic-embed-text) - About 300 mg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82d235-6241-4d90-bf61-8ab7f60f2d12",
   "metadata": {},
   "source": [
    "## Download the Models\n",
    "\n",
    "We need to download the models we will be using with Ollama.\n",
    "\n",
    "The cells below will do this from this Jupyter notebook.  When a Python notebook cell has a line starting with an ! or % that line is run as a command line.\n",
    "\n",
    "If you prefer you can open terminal or command prompt, run these commands:\n",
    "\n",
    "```bash\n",
    "ollama pull gemma:2b\n",
    "ollama pull mistral\n",
    "ollama pull llama3\n",
    "ollama pull nomic-embed-text\n",
    "ollama pull snowflake-arctic-embed\n",
    "```\n",
    "\n",
    "NOTE: if you have trouble with the ollama commands, most likely the Ollama directory is not in your PATH environment variable.\n",
    "You can check your path in the command line/terminal like this:\n",
    "\n",
    "For Windows:\n",
    "```bash\n",
    "echo %PATH% \n",
    "```\n",
    "For macOS/Linux:\n",
    "```bash\n",
    "echo $PATH\n",
    "```\n",
    "\n",
    "You can also browse for and try additional models at <https://ollama.com/library>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e870f507-b8a9-442a-b829-9ee777bf6b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import run\n",
    "\n",
    "\n",
    "def run_cmd_helper(cmd):\n",
    "    data = run(cmd, capture_output=True, shell=True)\n",
    "    output = data.stdout.decode('utf-8')\n",
    "    errors = data.stderr.decode('utf-8')\n",
    "    exit_code = data.returncode\n",
    "    return (exit_code, output, errors)\n",
    "    \n",
    "def run_cmd(cmd, quiet=True):\n",
    "    if not quiet:\n",
    "        print(f\"Running: {cmd}\")\n",
    "    ec, std_out, std_err = run_cmd_helper(cmd)\n",
    "    if ec != 0 or not quiet:\n",
    "        print(std_out, std_err)\n",
    "    return \"Success\" if ec == 0 else \"Failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e1a770-6b5e-444f-9dcf-7852e19b5b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: ollama --version\n",
      "ollama version is 0.1.32\n",
      " \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify Ollama is in the PATH and working.\n",
    "run_cmd(\"ollama --version\",quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "oyIDZLt5VBNq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oyIDZLt5VBNq",
    "outputId": "c3262af0-d9a2-42b3-81a2-5dd16145f216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling gemma:2b\n",
      "Pulling mistral\n",
      "Pulling nomic-embed-text\n",
      "Pulling snowflake-arctic-embed\n"
     ]
    }
   ],
   "source": [
    "MODELS = ['gemma:2b', 'mistral', 'nomic-embed-text', 'snowflake-arctic-embed']\n",
    "for model in MODELS:\n",
    "    print(f\"Pulling {model}\")\n",
    "    if not run_cmd(f\"ollama pull {model}\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "620c5d2e-6d97-428d-a78c-b878f5c49f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Success'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This one is larger, and optional.\n",
    "run_cmd('ollama pull llama3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef627181-7e11-4b5f-b089-47232df3975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: ollama list\n",
      "NAME                         \tID          \tSIZE  \tMODIFIED               \n",
      "gemma:2b                     \tb50d6c999e59\t1.7 GB\t38 seconds ago        \t\n",
      "llama3:latest                \ta6990ed6be41\t4.7 GB\tLess than a second ago\t\n",
      "mistral:latest               \t61e88e884507\t4.1 GB\t22 seconds ago        \t\n",
      "nomic-embed-text:latest      \t0a109f422b47\t274 MB\t20 seconds ago        \t\n",
      "snowflake-arctic-embed:latest\t21ab8b9b0545\t669 MB\t17 seconds ago        \t\n",
      " \n"
     ]
    }
   ],
   "source": [
    "run_cmd('ollama list',quiet=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tzG5G-DOfBB1",
   "metadata": {
    "id": "tzG5G-DOfBB1"
   },
   "source": [
    "## Test Ollama Endpoint\n",
    "\n",
    "This sends a request to the local Ollama server to verify it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3597f7ef-efbf-4a0b-affd-0b1a8dfe85b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"model\":\"gemma:2b\",\"created_at\":\"2024-04-27T22:33:55.7277255Z\",\"message\":{\"role\":\"assistant\",\"content\":\"The sky appears blue due to Rayleigh scattering. Rayleigh scattering is the scattering of light by particles smaller than the wavelength of light. Blue light has a shorter wavelength than other colors of light, so it is scattered more strongly by air molecules. This is why the sky appears blue.\"},\"done\":true,\"total_duration\":114053853000,\"load_duration\":7318479800,\"prompt_eval_count\":15,\"prompt_eval_duration\":11394857000,\"eval_count\":56,\"eval_duration\":95332457000}'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "# sends a simple request directly to the server\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "payload =  { \n",
    "    \"model\": \"gemma:2b\", \n",
    "    \"stream\": False, \n",
    "    \"messages\": [ { \"role\": \"user\", \"content\": \"Why is the sky blue?\" } ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json = payload)\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wxFKffg2fMYd",
   "metadata": {
    "id": "wxFKffg2fMYd"
   },
   "source": [
    "## Use LangChain library to interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "nxdgTIoWD7G8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxdgTIoWD7G8",
    "outputId": "ace1b8a4-1b28-4622-9908-1c1ba61dccb9"
   },
   "outputs": [],
   "source": [
    "# Try out Ollama with a model to verify working\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "# change the model name to try other models\n",
    "MODEL_ID = \"gemma:2b\"\n",
    "\n",
    "llm = Ollama(model=MODEL_ID, callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f58a108-ed8a-484b-8ee5-ce9548ed184d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are the first eight elements of the periodic table:\n",
      "\n",
      "| # | Element | Symbol | Atomic Number | Atomic Mass | Classification |\n",
      "|---|---|---|---|---|---|\n",
      "| 1 | Hydrogen | H | 1 | 1.008 | Nonmetal |\n",
      "| 2 | Helium | He | 2 | 4.0026 | Noble gas |\n",
      "| 3 | Lithium | Li | 3 | 6.941 | Alkali metal |\n",
      "| 4 | Beryllium | Be | 4 | 9.0122 | Alkaline earth metal |\n",
      "| 5 | Boron | B | 5 | 10.811 | Metalloid |\n",
      "| 6 | Carbon | C | 6 | 12.011 | Nonmetal |\n",
      "| 7 | Nitrogen | N | 7 | 14.007 | Nonmetal |\n",
      "| 8 | Oxygen | O | 8 | 15.999 | Nonmetal |"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure, here are the first eight elements of the periodic table:\\n\\n| # | Element | Symbol | Atomic Number | Atomic Mass | Classification |\\n|---|---|---|---|---|---|\\n| 1 | Hydrogen | H | 1 | 1.008 | Nonmetal |\\n| 2 | Helium | He | 2 | 4.0026 | Noble gas |\\n| 3 | Lithium | Li | 3 | 6.941 | Alkali metal |\\n| 4 | Beryllium | Be | 4 | 9.0122 | Alkaline earth metal |\\n| 5 | Boron | B | 5 | 10.811 | Metalloid |\\n| 6 | Carbon | C | 6 | 12.011 | Nonmetal |\\n| 7 | Nitrogen | N | 7 | 14.007 | Nonmetal |\\n| 8 | Oxygen | O | 8 | 15.999 | Nonmetal |'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What are the first eight elements of the periodic table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccc78ac-4753-4119-bbc4-7f9463f67ade",
   "metadata": {},
   "source": [
    "## Setup Chromadb Vectorstore\n",
    "\n",
    "Let'store some text embeddings in Chromadb to verify it is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114ad6c7-b22b-4335-b53c-f64c96c4fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need  some text documents.\n",
    "\n",
    "text1 = \"\"\"\n",
    "Gastroenterology is a branch of medicine that focuses on the digestive system and its disorders.\n",
    "It deals with the diagnosis, treatment, and prevention of diseases affecting the gastrointestinal tract,\n",
    "which includes organs such as the esophagus, stomach, small intestine, large intestine (colon),\n",
    "liver, pancreas, and gallbladder. Gastroenterologists are medical professionals who specialize in this\n",
    "field and are trained to perform various procedures like endoscopy and colonoscopy to examine and treat\n",
    "conditions related to the digestive system.\n",
    "\"\"\"\n",
    "\n",
    "text2 = \"\"\"\n",
    "Hepatology is a subspecialty of gastroenterology that focuses specifically on the study, diagnosis,\n",
    "management, and treatment of diseases related to the liver, biliary tract, and pancreas. This includes\n",
    "conditions such as hepatitis, cirrhosis, liver cancer, gallstones, and pancreatitis. Hepatologists are\n",
    "medical professionals who specialize in this field and possess extensive knowledge about the physiology\n",
    "and pathology of the liver and its associated organs.\n",
    "\"\"\"\n",
    "\n",
    "text3 = \"\"\"\n",
    "Some of the most common conditions that people see gastroenterologists for include:\n",
    "\n",
    "Gastroesophageal reflux disease (GERD)\n",
    "Irritable bowel syndrome (IBS)\n",
    "Inflammatory bowel disease (IBD), which includes Crohn's disease and ulcerative colitis\n",
    "Peptic ulcers\n",
    "Celiac disease\n",
    "Colorectal cancer screening and prevention\n",
    "Chronic constipation or diarrhea\n",
    "Gallbladder and bile duct disorders, such as gallstones or cholecystitis\n",
    "Liver diseases, including hepatitis, cirrhosis, and fatty liver disease\n",
    "Pancreatic diseases, such as pancreatitis or pancreatic cancer\n",
    "\"\"\"\n",
    "\n",
    "documents=[text1, text2, text3]\n",
    "titles=[\"gastroenterology\", \"hepatology\", \"conditions\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae53c12b-c187-4e07-9a62-d7319efe1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup embedding function using Ollama and create \n",
    "\n",
    "from chromadb.utils.embedding_functions import OllamaEmbeddingFunction\n",
    "\n",
    "# create EF with custom endpoint\n",
    "ef = OllamaEmbeddingFunction(\n",
    "    model_name=\"nomic-embed-text\",\n",
    "    url=\"http://localhost:11434/api/embeddings\",\n",
    ")\n",
    "\n",
    "# uncomment this if you want to see an example of the embedding function produces.\n",
    "#from pprint import pprint\n",
    "#pprint(ef([\"Where did you find that chrome plated llama?.\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0e380e2-b81a-42e5-91de-3b45da423130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " 'Some of the most common conditions that people see gastroenterologists for '\n",
      " 'include:\\n'\n",
      " '\\n'\n",
      " 'Gastroesophageal reflux disease (GERD)\\n'\n",
      " 'Irritable bowel syndrome (IBS)\\n'\n",
      " \"Inflammatory bowel disease (IBD), which includes Crohn's disease and \"\n",
      " 'ulcerative colitis\\n'\n",
      " 'Peptic ulcers\\n'\n",
      " 'Celiac disease\\n'\n",
      " 'Colorectal cancer screening and prevention\\n'\n",
      " 'Chronic constipation or diarrhea\\n'\n",
      " 'Gallbladder and bile duct disorders, such as gallstones or cholecystitis\\n'\n",
      " 'Liver diseases, including hepatitis, cirrhosis, and fatty liver disease\\n'\n",
      " 'Pancreatic diseases, such as pancreatitis or pancreatic cancer\\n']\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from pprint import pprint\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"ollama\")\n",
    "\n",
    "COLLECTION_NAME = \"my_collection\"\n",
    "try:\n",
    "    collection = client.get_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=ef\n",
    "    )\n",
    "except ValueError:\n",
    "    # We have not created the collection yet.\n",
    "    collection = client.create_collection(\n",
    "        name=COLLECTION_NAME,\n",
    "        embedding_function=ef,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "    )\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=[f\"id{i}\" for i in range(len(documents))],\n",
    "        metadatas=[{\"title\": t} for t in titles]\n",
    "    )\n",
    "results = collection.query(query_texts=[\"What are the most common GI conditions that require a doctor\"], n_results=1)\n",
    "pprint(results['documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dc30031-7505-4a54-9017-6197707c17d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': None,\n",
      " 'distances': [[0.46491573843456724]],\n",
      " 'documents': [['\\n'\n",
      "                'Hepatology is a subspecialty of gastroenterology that focuses '\n",
      "                'specifically on the study, diagnosis,\\n'\n",
      "                'management, and treatment of diseases related to the liver, '\n",
      "                'biliary tract, and pancreas. This includes\\n'\n",
      "                'conditions such as hepatitis, cirrhosis, liver cancer, '\n",
      "                'gallstones, and pancreatitis. Hepatologists are\\n'\n",
      "                'medical professionals who specialize in this field and '\n",
      "                'possess extensive knowledge about the physiology\\n'\n",
      "                'and pathology of the liver and its associated organs.\\n']],\n",
      " 'embeddings': None,\n",
      " 'ids': [['id1']],\n",
      " 'metadatas': [[{'title': 'hepatology'}]],\n",
      " 'uris': None}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6311d5ee-4f0d-40bc-b582-2cdf30b78b87",
   "metadata": {},
   "source": [
    "## Setup HuggingFace API\n",
    "\n",
    "You will need to create a HuggingFace Hub account unless you already have one.\n",
    "The prerequisite instructions cover how to do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "054f6638-50b2-4b61-a80b-b5832316a1dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "054f6638-50b2-4b61-a80b-b5832316a1dd",
    "outputId": "02eeb70c-b83c-4799-c521-f1c3787c6b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Hugging Face Hub token\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import toml\n",
    "\n",
    "token = None\n",
    "\n",
    "# check for a secrets toml file in the working directory with contents like:\n",
    "# HUGGINGFACE_TOKEN=\"your_actual_token_instead\"\n",
    "try:\n",
    "    with open('secrets.toml', 'r') as f:\n",
    "        config = toml.load(f)\n",
    "    if not 'HUGGINGFACE_TOKEN':\n",
    "        print(\"'HUGGINGFACE_TOKEN' not in secrets.toml\")\n",
    "    else:\n",
    "        token = config['HUGGINGFACE_TOKEN']\n",
    "except FileNotFoundError:\n",
    "    print(\"No secret file found\")\n",
    "\n",
    "\n",
    "if not token:\n",
    "    print(\"ERROR! No Hugging Face Hub API token found!\")\n",
    "else:\n",
    "    os.environ[\"HF_TOKEN\"] = token\n",
    "    print(\"Found Hugging Face Hub token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "kZ0DNnx6AwVw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZ0DNnx6AwVw",
    "outputId": "3c64197d-0fa2-40f7-859a-e16ec39b2119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\ranton\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "Isc_Xn5GAESg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Isc_Xn5GAESg",
    "outputId": "c5cf708b-2e8b-4907-d514-83e15f92b51e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Neon is a noble gas, which means it has a full valence shell and is therefore relatively unreactive. The neon gases are all noble gases, and their atomic numbers are as follows:\n",
      "\n",
      "1. Neon (Ne) - atomic number 10\n",
      "2. Helium (He) - atomic number 2\n",
      "3. Argon (Ar) - atomic number 18\n",
      "4. Krypton (Kr) - atomic number 36\n",
      "5. Xenon (Xe) - atomic number 54\n",
      "\n",
      "Therefore, the neon gases in order of their atomic numbers are:\n",
      "\n",
      "1. Helium (He)\n",
      "2. Neon (Ne)\n",
      "3. Argon (Ar)\n",
      "4. Krypton (Kr)\n",
      "5. Xenon (Xe)\n",
      "\n",
      "However, neon is not commonly used as a neon gas in neon signs or lighting due to its low vapor pressure at standard temperatures and pressures. Instead, neon lamps typically use a mixture of neon and other noble gases, such as argon or helium, to improve their performance.</s>"
     ]
    }
   ],
   "source": [
    "# call inference endpoint to verify token is working\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "HF_MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "# You can find other models to try at https://huggingface.co/models.\n",
    "    \n",
    "client = InferenceClient(model=HF_MODEL_ID, token=token)\n",
    "\n",
    "prompt=\"List the neon gases in order of their atomic numbers.\"\n",
    "for out_token in client.text_generation(prompt=prompt, max_new_tokens=512, stream=True):\n",
    "    print(out_token, end='')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "020762fca3134c48bd6d71c1e984af68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "0ca0c03fb4db422eb54fce10fd25f732": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6db94c206aaa48e58de6c0a73a2964be",
      "placeholder": "​",
      "style": "IPY_MODEL_a520d7581a8d473584794a7032f6e33a",
      "value": ""
     }
    },
    "300335257bc84a1a8b724a072449522b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_020762fca3134c48bd6d71c1e984af68",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63dadd4795744fa5a78eb12d164ecb4a",
      "value": 0
     }
    },
    "594a3a94cc084792aff215d987774fed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62aee4875f5149bc950bb78198636ee7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63dadd4795744fa5a78eb12d164ecb4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6db94c206aaa48e58de6c0a73a2964be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7208850129be48f6a0e2a507af80b078": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_594a3a94cc084792aff215d987774fed",
      "placeholder": "​",
      "style": "IPY_MODEL_fd7a39e56aac4b58af367567ff576ffb",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "7a976e06f04942fb9e277acbf1681948": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0ca0c03fb4db422eb54fce10fd25f732",
       "IPY_MODEL_300335257bc84a1a8b724a072449522b",
       "IPY_MODEL_7208850129be48f6a0e2a507af80b078"
      ],
      "layout": "IPY_MODEL_62aee4875f5149bc950bb78198636ee7"
     }
    },
    "a520d7581a8d473584794a7032f6e33a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd7a39e56aac4b58af367567ff576ffb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
